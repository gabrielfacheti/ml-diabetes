{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'>Getting Started </span></b> </div>\n\n<br>\n\nThe aim of this analysis is to investigate a range of health-related factors and their interconnections **to classify diabetes accurately**. These factors include aspects such as **`age`**, **`gender`**, **`body mass index (BMI)`**, **`hypertension`**, **`heart disease`**, **`smoking history`**, **`HbA1c level`**, and **`blood glucose level`**. This comprehensive examination will not only provide insights into the patterns and trends in diabetes risk but will also create a solid base for further research. Specifically, research can be built on how these variables interact and influence diabetes occurrence and progression, crucial knowledge for improving patient care and outcomes in this increasingly critical area of healthcare.\n\n<br>\n\n![](https://www.aoa.org/AOA/Images/News_2019/Donut-or-orange.jpg)\n\n<br>\n\n### <b><span style='color:#16C2D5'>|</span> Domain Knowledge</b>\n\n<br>\n\n\n1. **`Age`:** Age is an important factor in predicting diabetes risk. As individuals get older, their risk of developing diabetes increases. This is partly due to factors such as reduced physical activity, changes in hormone levels, and a higher likelihood of developing other health conditions that can contribute to diabetes.\n\n2. **`Gender`**: Gender can play a role in diabetes risk, although the effect may vary. For example, women with a history of gestational diabetes (diabetes during pregnancy) have a higher risk of developing type 2 diabetes later in life. Additionally, some studies have suggested that men may have a slightly higher risk of diabetes compared to women.\n\n3. **`Body Mass Index (BMI)`:** BMI is a measure of body fat based on a person's height and weight. It is commonly used as an indicator of overall weight status and can be helpful in predicting diabetes risk. Higher BMI is associated with a greater likelihood of developing type 2 diabetes. Excess body fat, particularly around the waist, can lead to insulin resistance and impair the body's ability to regulate blood sugar levels.\n\n4. **`Hypertension`:** Hypertension, or high blood pressure, is a condition that often coexists with diabetes. The two conditions share common risk factors and can contribute to each other's development. Having hypertension increases the risk of developing type 2 diabetes and vice versa. Both conditions can have detrimental effects on cardiovascular health.\n\n5. **`Heart Disease`:** Heart disease, including conditions such as coronary artery disease and heart failure, is associated with an increased risk of diabetes. The relationship between heart disease and diabetes is bidirectional, meaning that having one condition increases the risk of developing the other. This is because they share many common risk factors, such as obesity, high blood pressure, and high cholesterol.\n\n6. **`Smoking History`:** Smoking is a modifiable risk factor for diabetes. Cigarette smoking has been found to increase the risk of developing type 2 diabetes. Smoking can contribute to insulin resistance and impair glucose metabolism. Quitting smoking can significantly reduce the risk of developing diabetes and its complications.\n\n7. **`HbA1c Level`:** HbA1c (glycated hemoglobin) is a measure of the average blood glucose level over the past 2-3 months. It provides information about long-term blood sugar control. Higher HbA1c levels indicate poorer glycemic control and are associated with an increased risk of developing diabetes and its complications.\n\n8. **`Blood Glucose Level`:** Blood glucose level refers to the amount of glucose (sugar) present in the blood at a given time. Elevated blood glucose levels, particularly in the fasting state or after consuming carbohydrates, can indicate impaired glucose regulation and increase the risk of developing diabetes. Regular monitoring of blood glucose levels is important in the diagnosis and management of diabetes.\n\n<br>\n\n✔️ **These features, when combined and analyzed with appropriate statistical and machine learning techniques, can help in predicting an individual's risk of developing diabetes.**","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#E888BB; font-size: 1%;\">INTRODUCTION</span>\n<div style=\"padding: 35px;color:white;margin:10;font-size:170%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://images.pexels.com/photos/3850689/pexels-photo-3850689.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\"><b><span style='color:black'>INTRODUCTION </span></b> </div>","metadata":{}},{"cell_type":"markdown","source":"### <b>I <span style='color:#16C2D5'>|</span> Preface</b> \n\nIn this analysis, we have chosen the RandomForest classifier as our model. **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">The RandomForest algorithm</mark>** is an ensemble learning method that operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes for classification or mean prediction of the individual trees for regression.\n\nSeveral reasons guided our choice of **<span style='color:#16C2D5'>Random Forest</span>** for this task:\n\n1. **Handling of Large Data:** **<span style='color:#16C2D5'>Random Forest</span>** is capable of efficiently handling large datasets with high dimensionality. Our dataset, containing a substantial number of rows and several features, falls into this category.\n\n2. **Robustness to Overfitting:** **<span style='color:#16C2D5'>Random Forest</span>** reduces the risk of overfitting, which is a common problem with decision trees. The algorithm accomplishes this by creating a set of **<mark style=\"background-color:#DDE6ED;color:black;border-radius:4px;opacity:1.0\">decision trees</mark>** (a \"forest\") and making the final prediction based on the majority vote of the individual trees.\n\n3. **Handling Mixed Data Types:** In our dataset, we have both numerical and categorical features. **<span style='color:#16C2D5'>Random Forest</span>** handles such mixtures smoothly, which makes it an ideal choice.\n\n4. **Feature Importance:** **<span style='color:#16C2D5'>Random Forest</span>** provides a straightforward way to estimate feature importance. Given our aim to investigate the impact of different factors on diabetes, this characteristic is particularly useful.\n\n5. **Non-linearity:** Medical data often contains complex and non-linear relationships. **<span style='color:#16C2D5'>Random Forest</span>**, being a non-linear model, can capture these relationships effectively.\n\n<br>\n\n> ⚠️ It's worth noting that while **<span style='color:#16C2D5'>Random Forest</span>** is a strong candidate given its mentioned advantages, the choice of model should always be considered with a grain of salt. Other models might perform better on the task, and it's generally a good practice to try several models and compare their performance. However, for the purpose of this analysis and given our dataset, **<span style='color:#16C2D5'>Random Forest</span>** **is a practical and reasonable starting point**.","metadata":{}},{"cell_type":"markdown","source":"### <b>II <span style='color:#16C2D5'>|</span> Import libraries</b> ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Import Neccessary libraries\nimport numpy as np \nimport pandas as pd \n\n# Import Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Import Model\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\n#Import Sampler libraries\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as imbPipeline\n\n# Set the decimal format\npd.options.display.float_format = \"{:.2f}\".format","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-25T03:55:44.844829Z","iopub.execute_input":"2023-05-25T03:55:44.846174Z","iopub.status.idle":"2023-05-25T03:55:47.234529Z","shell.execute_reply.started":"2023-05-25T03:55:44.846121Z","shell.execute_reply":"2023-05-25T03:55:47.233225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>III <span style='color:#16C2D5'>|</span> Input the data</b> ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/diabetes-prediction-dataset/diabetes_prediction_dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.237097Z","iopub.execute_input":"2023-05-25T03:55:47.237878Z","iopub.status.idle":"2023-05-25T03:55:47.459124Z","shell.execute_reply.started":"2023-05-25T03:55:47.23781Z","shell.execute_reply":"2023-05-25T03:55:47.457794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.461745Z","iopub.execute_input":"2023-05-25T03:55:47.462139Z","iopub.status.idle":"2023-05-25T03:55:47.500228Z","shell.execute_reply.started":"2023-05-25T03:55:47.462104Z","shell.execute_reply":"2023-05-25T03:55:47.499001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E888BB; font-size: 1%;\">1 | EXPLORATORY DATA ANALYSIS</span>\n<div style=\"padding: 35px;color:white;margin:10;font-size:170%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://images.pexels.com/photos/3850689/pexels-photo-3850689.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\"><b><span style='color:black'>1 | EXPLORATORY DATA ANALYSIS </span></b> </div>\n\n## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 1. Data Cleansing</span></b> </div>\n\n### <b>I <span style='color:#16C2D5'>|</span> Handling Duplicates</b> ","metadata":{}},{"cell_type":"code","source":"# Handle duplicates\nduplicate_rows_data = df[df.duplicated()]\nprint(\"number of duplicate rows: \", duplicate_rows_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.503677Z","iopub.execute_input":"2023-05-25T03:55:47.504189Z","iopub.status.idle":"2023-05-25T03:55:47.564237Z","shell.execute_reply.started":"2023-05-25T03:55:47.504141Z","shell.execute_reply":"2023-05-25T03:55:47.562992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.566001Z","iopub.execute_input":"2023-05-25T03:55:47.566439Z","iopub.status.idle":"2023-05-25T03:55:47.610872Z","shell.execute_reply.started":"2023-05-25T03:55:47.566396Z","shell.execute_reply":"2023-05-25T03:55:47.60978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>II <span style='color:#16C2D5'>|</span> Uniqueness</b> ","metadata":{}},{"cell_type":"code","source":"# Loop through each column and count the number of distinct values\nfor column in df.columns:\n    num_distinct_values = len(df[column].unique())\n    print(f\"{column}: {num_distinct_values} distinct values\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.612742Z","iopub.execute_input":"2023-05-25T03:55:47.613487Z","iopub.status.idle":"2023-05-25T03:55:47.646812Z","shell.execute_reply.started":"2023-05-25T03:55:47.61343Z","shell.execute_reply":"2023-05-25T03:55:47.645593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>III <span style='color:#16C2D5'>|</span> Missing Values</b> ","metadata":{}},{"cell_type":"code","source":"# Checking null values\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.648436Z","iopub.execute_input":"2023-05-25T03:55:47.649228Z","iopub.status.idle":"2023-05-25T03:55:47.738987Z","shell.execute_reply.started":"2023-05-25T03:55:47.64919Z","shell.execute_reply":"2023-05-25T03:55:47.737783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Unneccessary value [0.00195%]\ndf = df[df['gender'] != 'Other']","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.740818Z","iopub.execute_input":"2023-05-25T03:55:47.742115Z","iopub.status.idle":"2023-05-25T03:55:47.778379Z","shell.execute_reply.started":"2023-05-25T03:55:47.742064Z","shell.execute_reply":"2023-05-25T03:55:47.777163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>IV <span style='color:#16C2D5'>|</span> Describe the Data</b> ","metadata":{}},{"cell_type":"code","source":"df.describe().style.format(\"{:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.780015Z","iopub.execute_input":"2023-05-25T03:55:47.780484Z","iopub.status.idle":"2023-05-25T03:55:47.937464Z","shell.execute_reply.started":"2023-05-25T03:55:47.780448Z","shell.execute_reply":"2023-05-25T03:55:47.936334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 2. Univariative Analysis</span></b> </div>\n\n### <b>I <span style='color:#16C2D5'>|</span> Histogram for age</b> ","metadata":{}},{"cell_type":"code","source":"# Histogram for age\nplt.hist(df['age'], bins=30, edgecolor='black')\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:47.943247Z","iopub.execute_input":"2023-05-25T03:55:47.943996Z","iopub.status.idle":"2023-05-25T03:55:48.301903Z","shell.execute_reply.started":"2023-05-25T03:55:47.943948Z","shell.execute_reply":"2023-05-25T03:55:48.300791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>II <span style='color:#16C2D5'>|</span> Bar plot for gender</b> ","metadata":{}},{"cell_type":"code","source":"# Bar plot for gender\nsns.countplot(x='gender', data=df)\nplt.title('Gender Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:48.303522Z","iopub.execute_input":"2023-05-25T03:55:48.30473Z","iopub.status.idle":"2023-05-25T03:55:48.614933Z","shell.execute_reply.started":"2023-05-25T03:55:48.304657Z","shell.execute_reply":"2023-05-25T03:55:48.613795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>III <span style='color:#16C2D5'> | </span> Distribution plot for BMI</b> ","metadata":{}},{"cell_type":"code","source":"# Distribution plot for BMI\nsns.distplot(df['bmi'], bins=30)\nplt.title('BMI Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:48.616777Z","iopub.execute_input":"2023-05-25T03:55:48.617476Z","iopub.status.idle":"2023-05-25T03:55:49.524151Z","shell.execute_reply.started":"2023-05-25T03:55:48.617432Z","shell.execute_reply":"2023-05-25T03:55:49.523045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>IV <span style='color:#16C2D5'>|</span> Count plots for binary variables</b> ","metadata":{}},{"cell_type":"code","source":"# Count plots for binary variables\nfor col in ['hypertension', 'heart_disease', 'diabetes']:\n    sns.countplot(x=col, data=df)\n    plt.title(f'{col} Distribution')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:49.525615Z","iopub.execute_input":"2023-05-25T03:55:49.526109Z","iopub.status.idle":"2023-05-25T03:55:50.102613Z","shell.execute_reply.started":"2023-05-25T03:55:49.526073Z","shell.execute_reply":"2023-05-25T03:55:50.101456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>V <span style='color:#16C2D5'>|</span> Count plot for smoking history</b> ","metadata":{}},{"cell_type":"code","source":"# Count plot for smoking history\nsns.countplot(x='smoking_history', data=df)\nplt.title('Smoking History Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:50.104086Z","iopub.execute_input":"2023-05-25T03:55:50.10443Z","iopub.status.idle":"2023-05-25T03:55:50.454199Z","shell.execute_reply.started":"2023-05-25T03:55:50.1044Z","shell.execute_reply":"2023-05-25T03:55:50.452768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 3.Bivariative Analysis</span></b> </div>\n### <b>I <span style='color:#16C2D5'>|</span> Boxplot BMI vs Diabetes classification</b> ","metadata":{}},{"cell_type":"code","source":"# Boxplot BMI vs Diabetes classification\nsns.boxplot(x='diabetes', y='bmi', data=df)\nplt.title('BMI vs Diabetes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:50.455898Z","iopub.execute_input":"2023-05-25T03:55:50.456289Z","iopub.status.idle":"2023-05-25T03:55:50.682279Z","shell.execute_reply.started":"2023-05-25T03:55:50.456253Z","shell.execute_reply":"2023-05-25T03:55:50.681312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>II <span style='color:#16C2D5'>|</span> Boxplot Age vs Diabetes classification</b> ","metadata":{}},{"cell_type":"code","source":"# Boxplot Age vs Diabetes classification\nsns.boxplot(x='diabetes', y='age', data=df)\nplt.title('Age vs Diabetes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:50.684199Z","iopub.execute_input":"2023-05-25T03:55:50.684986Z","iopub.status.idle":"2023-05-25T03:55:50.936212Z","shell.execute_reply.started":"2023-05-25T03:55:50.684944Z","shell.execute_reply":"2023-05-25T03:55:50.93501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>III <span style='color:#16C2D5'>|</span> Count plot of gender vs diabetes</b> ","metadata":{}},{"cell_type":"code","source":"# Count plot of gender vs diabetes\nsns.countplot(x='gender', hue='diabetes', data=df)\nplt.title('Gender vs Diabetes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:50.938115Z","iopub.execute_input":"2023-05-25T03:55:50.938556Z","iopub.status.idle":"2023-05-25T03:55:51.253886Z","shell.execute_reply.started":"2023-05-25T03:55:50.938514Z","shell.execute_reply":"2023-05-25T03:55:51.252569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>III <span style='color:#16C2D5'>|</span> Boxplot HbA1c level vs Diabetes classification</b> ","metadata":{}},{"cell_type":"code","source":"# Boxplot HbA1c level vs Diabetes classification\nsns.boxplot(x='diabetes', y='HbA1c_level', data=df)\nplt.title('HbA1c level vs Diabetes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:51.256203Z","iopub.execute_input":"2023-05-25T03:55:51.256658Z","iopub.status.idle":"2023-05-25T03:55:51.448877Z","shell.execute_reply.started":"2023-05-25T03:55:51.256614Z","shell.execute_reply":"2023-05-25T03:55:51.447773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>IV <span style='color:#16C2D5'>|</span> Boxplot blood glucose level vs Diabetes classification</b> ","metadata":{}},{"cell_type":"code","source":"# Boxplot blood glucose level vs Diabetes classification\nsns.boxplot(x='diabetes', y='blood_glucose_level', data=df)\nplt.title('Blood Glucose Level vs Diabetes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:51.450729Z","iopub.execute_input":"2023-05-25T03:55:51.45157Z","iopub.status.idle":"2023-05-25T03:55:51.632032Z","shell.execute_reply.started":"2023-05-25T03:55:51.451522Z","shell.execute_reply":"2023-05-25T03:55:51.630757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>V <span style='color:#16C2D5'>|</span> Pair plot for numeric features</b> ","metadata":{}},{"cell_type":"code","source":"# Pair plot for numeric features\nsns.pairplot(df, hue='diabetes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:55:51.63382Z","iopub.execute_input":"2023-05-25T03:55:51.634304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 4. Multivariative analysis</span></b> </div>\n\n### <b>I <span style='color:#16C2D5'>|</span> Scatterplot Age vs BMI colored by Diabetes classification</b> ","metadata":{}},{"cell_type":"code","source":"# Scatterplot Age vs BMI colored by Diabetes classification\nsns.scatterplot(x='age', y='bmi', hue='diabetes', data=df)\nplt.title('Age vs BMI')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>II <span style='color:#16C2D5'>|</span> Violin plot of BMI against diabetes classification split by gender</b> ","metadata":{}},{"cell_type":"code","source":"# Violin plot of BMI against diabetes classification split by gender\nsns.violinplot(x='diabetes', y='bmi', hue='gender', split=True, data=df)\nplt.title('BMI vs Diabetes split by Gender')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-25T03:59:21.045197Z","iopub.status.idle":"2023-05-25T03:59:21.832969Z","shell.execute_reply.started":"2023-05-25T03:59:21.045162Z","shell.execute_reply":"2023-05-25T03:59:21.831831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>III <span style='color:#16C2D5'>|</span> Interaction between gender, BMI and diabetes</b> ","metadata":{}},{"cell_type":"code","source":"# Interaction between gender, BMI and diabetes\nsns.boxplot(x='diabetes', y='bmi', hue='gender', data=df)\nplt.title('BMI Distribution by Diabetes Status and Gender')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:21.834484Z","iopub.execute_input":"2023-05-25T03:59:21.83488Z","iopub.status.idle":"2023-05-25T03:59:22.321724Z","shell.execute_reply.started":"2023-05-25T03:59:21.834846Z","shell.execute_reply":"2023-05-25T03:59:22.320516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>IV <span style='color:#16C2D5'>|</span> Interaction between gender, Age and diabetes</b> ","metadata":{}},{"cell_type":"code","source":"# Interaction between gender, Age and diabetes\nsns.boxplot(x='diabetes', y='age', hue='gender', data=df)\nplt.title('Age Distribution by Diabetes Status and Gender')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:22.324459Z","iopub.execute_input":"2023-05-25T03:59:22.324955Z","iopub.status.idle":"2023-05-25T03:59:22.813406Z","shell.execute_reply.started":"2023-05-25T03:59:22.32491Z","shell.execute_reply":"2023-05-25T03:59:22.811959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E888BB; font-size: 1%;\">2 | CORRELATION</span>\n<div style=\"padding: 35px;color:white;margin:10;font-size:170%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://images.pexels.com/photos/3850689/pexels-photo-3850689.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\"><b><span style='color:black'>2 | CORRELATION </span></b> </div>\n\n### <b>I <span style='color:#16C2D5'>|</span> Data preparation</b> ","metadata":{}},{"cell_type":"code","source":"# Define a function to map the existing categories to new ones\ndef recategorize_smoking(smoking_status):\n    if smoking_status in ['never', 'No Info']:\n        return 'non-smoker'\n    elif smoking_status == 'current':\n        return 'current'\n    elif smoking_status in ['ever', 'former', 'not current']:\n        return 'past_smoker'\n\n# Apply the function to the 'smoking_history' column\ndf['smoking_history'] = df['smoking_history'].apply(recategorize_smoking)\n\n# Check the new value counts\nprint(df['smoking_history'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:22.814902Z","iopub.execute_input":"2023-05-25T03:59:22.815624Z","iopub.status.idle":"2023-05-25T03:59:22.878867Z","shell.execute_reply.started":"2023-05-25T03:59:22.815568Z","shell.execute_reply":"2023-05-25T03:59:22.877557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df.copy()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:22.88048Z","iopub.execute_input":"2023-05-25T03:59:22.880833Z","iopub.status.idle":"2023-05-25T03:59:22.891451Z","shell.execute_reply.started":"2023-05-25T03:59:22.880802Z","shell.execute_reply":"2023-05-25T03:59:22.890364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>II <span style='color:#16C2D5'>|</span> Encoding</b> ","metadata":{}},{"cell_type":"code","source":"def perform_one_hot_encoding(df, column_name):\n    # Perform one-hot encoding on the specified column\n    dummies = pd.get_dummies(df[column_name], prefix=column_name)\n\n    # Drop the original column and append the new dummy columns to the dataframe\n    df = pd.concat([df.drop(column_name, axis=1), dummies], axis=1)\n\n    return df\n\n# Perform one-hot encoding on the gender variable\ndata = perform_one_hot_encoding(data, 'gender')\n\n# Perform one-hot encoding on the smoking history variable\ndata = perform_one_hot_encoding(data, 'smoking_history')","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:22.892861Z","iopub.execute_input":"2023-05-25T03:59:22.893252Z","iopub.status.idle":"2023-05-25T03:59:22.945437Z","shell.execute_reply.started":"2023-05-25T03:59:22.893221Z","shell.execute_reply":"2023-05-25T03:59:22.944225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>III <span style='color:#16C2D5'>|</span> Correlation Matrix</b> ","metadata":{}},{"cell_type":"code","source":"# Compute the correlation matrix\ncorrelation_matrix = data.corr()\n#Graph I.\nplt.figure(figsize=(15, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\nplt.title(\"Correlation Matrix Heatmap\")\nplt.show()\n\n\n#Graph II\n# Create a heatmap of the correlations with the target column\ncorr = data.corr()\ntarget_corr = corr['diabetes'].drop('diabetes')\n\n# Sort correlation values in descending order\ntarget_corr_sorted = target_corr.sort_values(ascending=False)\n\nsns.set(font_scale=0.8)\nsns.set_style(\"white\")\nsns.set_palette(\"PuBuGn_d\")\nsns.heatmap(target_corr_sorted.to_frame(), cmap=\"coolwarm\", annot=True, fmt='.2f')\nplt.title('Correlation with Diabetes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:22.951304Z","iopub.execute_input":"2023-05-25T03:59:22.951697Z","iopub.status.idle":"2023-05-25T03:59:24.340914Z","shell.execute_reply.started":"2023-05-25T03:59:22.951662Z","shell.execute_reply":"2023-05-25T03:59:24.339793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E888BB; font-size: 1%;\">3 |  PREDICTIVE ANALYSIS</span>\n<div style=\"padding: 35px;color:white;margin:10;font-size:170%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://images.pexels.com/photos/3850689/pexels-photo-3850689.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\"><b><span style='color:black'>3 |  PREDICTIVE ANALYSIS </span></b> </div>\n\n## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 1. Class Imbalance</span></b> </div>\n\nFrom the EDA ,the dataset is imbalanced (with 9% positive cases for diabetes and 91% negative cases), it's essential to balance the data to ensure that the model doesn't get biased towards the majority class. For this purpose, the Synthetic Minority Over-sampling Technique (SMOTE) is used, which generates synthetic samples for the minority class.","metadata":{}},{"cell_type":"code","source":"# Count plot for the 'diabetes' variable\nsns.countplot(x='diabetes', data=df)\nplt.title('Diabetes Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:24.342243Z","iopub.execute_input":"2023-05-25T03:59:24.343077Z","iopub.status.idle":"2023-05-25T03:59:24.58899Z","shell.execute_reply.started":"2023-05-25T03:59:24.343039Z","shell.execute_reply":"2023-05-25T03:59:24.587897Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define resampling\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:24.59038Z","iopub.execute_input":"2023-05-25T03:59:24.590874Z","iopub.status.idle":"2023-05-25T03:59:24.596675Z","shell.execute_reply.started":"2023-05-25T03:59:24.590839Z","shell.execute_reply":"2023-05-25T03:59:24.595513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 2.Preprocessing : Scaler</span></b> </div>\n\nPreprocessing is a crucial step before training the model. In this case, numerical features are standardized (mean removed and scaled to unit variance), and categorical features are one-hot encoded. **<span style='color:#16C2D5'>Standardization</span>** is not required for all models but is generally a good practice. **<span style='color:#16C2D5'>One-hot encoding</span>** is necessary for categorical variables to be correctly understood by the machine learning model.\n\nThe **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">StandardScaler</mark>** in sklearn is based on the assumption that the data, Y, follows a distribution that might not necessarily be Gaussian (normal), but we still transform it in a way that its distribution will have a mean value 0 and standard deviation of 1.</p>\n\n<p>In other words, given a feature vector <em>x</em>, it modifies the values as follows:</p>\n\n<p class=\"formulaDsp\">\n\\[ Y_i = \\frac{x_i - \\mu(\\vec{x})}{\\sigma(\\vec{x})} \\]\n</p>\n\n**where:**\n<ul>\n<li>\\( x_i \\) is the i-th element of the original feature vector \\( \\vec{x} \\),</li>\n<li>\\( \\mu(\\vec{x}) \\) is the mean of the feature vector, and</li>\n<li>\\( \\sigma(\\vec{x}) \\) is the standard deviation of the feature vector.</li>\n</ul>\n\n<p>The transformed data \\( Y \\) (each \\( Y_i \\)) will have properties such that \\( mean(Y) = 0 \\) and \\( std(Y) = 1 \\).</p>\n\n> This transformation is also known as Z-score normalization.\n","metadata":{}},{"cell_type":"code","source":"# Define preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level','hypertension','heart_disease']),\n        ('cat', OneHotEncoder(), ['gender','smoking_history'])\n    ])\n\n# Split data into features and target variable\nX = df.drop('diabetes', axis=1)\ny = df['diabetes']","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:24.598453Z","iopub.execute_input":"2023-05-25T03:59:24.598816Z","iopub.status.idle":"2023-05-25T03:59:24.617437Z","shell.execute_reply.started":"2023-05-25T03:59:24.598783Z","shell.execute_reply":"2023-05-25T03:59:24.616202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a pipeline that preprocesses the data, resamples data, and then trains a classifier\nclf = imbPipeline(steps=[('preprocessor', preprocessor),\n                      ('over', over),\n                      ('under', under),\n                      ('classifier', RandomForestClassifier())])","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:24.618952Z","iopub.execute_input":"2023-05-25T03:59:24.619308Z","iopub.status.idle":"2023-05-25T03:59:24.627055Z","shell.execute_reply.started":"2023-05-25T03:59:24.619276Z","shell.execute_reply":"2023-05-25T03:59:24.626225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 3. Model Building and Hyperparameter Tuning</span></b> </div>\n\nA pipeline is constructed which first applies the preprocessing steps and then trains a model on the data. We use a **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">RandomForestClassifier</mark>**, which is a popular and powerful algorithm for classification tasks. The model's hyperparameters are tuned using **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">GridSearchCV</mark>** , which performs an exhaustive search over the specified parameter values for the estimator. The best performing model is selected based on cross-validation.","metadata":{}},{"cell_type":"code","source":"# Define the hyperparameters and the values we want to test\nparam_grid = {\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__max_depth': [None, 10, 20],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4]\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:24.628275Z","iopub.execute_input":"2023-05-25T03:59:24.628632Z","iopub.status.idle":"2023-05-25T03:59:24.638745Z","shell.execute_reply.started":"2023-05-25T03:59:24.628574Z","shell.execute_reply":"2023-05-25T03:59:24.637721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Grid Search object\ngrid_search = GridSearchCV(clf, param_grid, cv=5)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters\nprint(\"Best Parameters: \", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T03:59:24.640031Z","iopub.execute_input":"2023-05-25T03:59:24.640368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#16C2D5'>|</span> Intepret the results </b>\n\nThe result shows the best parameters for our **<span style='color:#16C2D5'>Random Forest model</span>** that were found during the hyperparameter tuning process:\n\n1. **max_depth of 10:** This indicates that the maximum depth of the trees in the forest is 10 levels. Constraining the depth of the tree helps in reducing overfitting. It appears from this result that a medium-complexity tree works best for our data. Too much complexity (a deeper tree) may capture noise, and too little (a shallower tree) may not capture the underlying structure of the data.\n\n2. **min_samples_leaf of 2:** This means that each leaf (the end node of a decision tree, where predictions are made) must contain at least two samples. This parameter, like max_depth, is a way to control overfitting. By requiring at least two samples to make a prediction, the model prevents fitting to outliers or noise in the training data.\n\n3. **min_samples_split of 2:** This tells us that a node must contain at least two samples in order to be split (to create two child nodes). Similar to the min_samples_leaf parameter, this can help control overfitting.\n\n4. **n_estimators of 50:** This is the number of decision trees in the forest. The Random Forest algorithm works by averaging the predictions of many decision trees to make a final prediction, which helps reduce overfitting and variance. In this case, it seems that having 50 trees in the forest gives us the best performance.\n\n<br>\n\n<div style=\"border-radius:10px;border:#16C2D5 solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\">\n     💬 These parameters are a result of the Hyperparameter tuning process , and they give us insight into the structure of the data and the complexity of the model that best captures that structure. The moderately constrained tree depth and the requirements for the number of samples at each node suggest a model that is complex enough to capture the important patterns in the data, but not so complex that it overfits to noise or outliers.This balance is crucial in creating a model that will generalize well to new data.\n\n</div>\n \n    \n    ","metadata":{}},{"cell_type":"markdown","source":"![](https://digitalpress.fra1.cdn.digitaloceanspaces.com/mhujhsj/2022/09/Internal-Nodes-crop-1.png)\n\n<br>\n\n> Remember, these are the optimal parameters given the parameter grid we defined and the specific dataset at hand. For a **different dataset** or with a **different parameter grid**, **the optimal parameters could be different.**","metadata":{}},{"cell_type":"code","source":"# Convert GridSearchCV results to a DataFrame and plot\nresults_df = pd.DataFrame(grid_search.cv_results_)\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=results_df, x='param_classifier__n_estimators', y='mean_test_score', hue='param_classifier__max_depth', palette='viridis')\nplt.title('Hyperparameters Tuning Results')\nplt.xlabel('Number of Estimators')\nplt.ylabel('Mean Test Score')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 4. Confusion Matrix</span></b> </div>\n\nThe trained model is evaluated on the **test set**. **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">Confusion matrix</mark>** is used to visualize the performance of the model. It shows the true positive, true negative, false positive, and false negative predictions of the model.\n\n<br>\n\n![](https://miro.medium.com/v2/resize:fit:1218/1*jMs1RmSwnYgR9CsBw-z1dw.png)\n\n#### **<span style='color:#16C2D5'>Precision:</span>**\n\nPrecision is a measure of how many of the true positive predictions were actually correct. It is defined as the number of true positives (TP) divided by the sum of true positives (TP) and false positives (FP).\n\n<p class=\"formulaDsp\">\n\\[ Precision = \\frac{TP}{TP + FP} \\]\n</p>\n\n#### **<span style='color:#16C2D5'>Recall:</span>**\n\n\nRecall (or Sensitivity) is a measure of how many of the actual positive cases were identified correctly. It is defined as the number of true positives (TP) divided by the sum of true positives (TP) and false negatives (FN).\n\n<br>\n\n<p class=\"formulaDsp\">\n\\[ Recall = \\frac{TP}{TP + FN} \\]\n</p>\n\n#### **<span style='color:#16C2D5'>F1-Score:</span>**\n\n\nThe F1 score is the harmonic mean of Precision and Recall and tries to find the balance between precision and recall. It is defined as 2 times the product of precision and recall divided by the sum of precision and recall.\n\n<br>\n\n<p class=\"formulaDsp\">\n\\[ F1 Score = \\frac{2 * Precision * Recall}{Precision + Recall} \\]\n</p>\n\n<br>\n    \n<div style=\"border-radius:10px;border:#16C2D5 solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\">     \n    <b><span style='color:#16C2D5'>|</span> In all of these formulas: </b>\n    \nTrue Positives (TP) are the cases in which we predicted yes (diabetes present), and the actual was also yes.\nTrue Negatives (TN) are the cases in which we predicted no, and the actual was also no.\nFalse Positives (FP) are the cases in which we predicted yes, but the actual was no.\nFalse Negatives (FN) are the cases in which we predicted no, but the actual was yes.   \n</div>  \n","metadata":{}},{"cell_type":"code","source":"# Predict on the test set using the best model\ny_pred = grid_search.predict(X_test)\n\n# Evaluate the model\nprint(\"Model Accuracy: \", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#16C2D5'>|</span> Intepret the results </b>\n\nOur trained **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">Random Forest Model</mark>** achieved an accuracy of around 95%. This indicates that the model correctly classified around 95% of all cases in the test set.\n\nLooking deeper into the classification metrics, let's dissect the performance for each class (0 and 1) separately:\n\n#### <b><span style='color:#16C2D5'> A |</span> Class 0 (Non-diabetes): </b>\n- The model has a high precision (0.98) for class 0, meaning that among all instances where the model predicted non-diabetes, 98% were indeed non-diabetes. \n- The recall for class 0 is also high (0.96). This means that our model correctly identified 96% of all actual non-diabetes cases in the dataset. \n\n#### <b><span style='color:#16C2D5'> B |</span> Class 1 (Diabetes): </b>\n- The precision for class 1 is lower around (0.65), which indicates that when the model predicted diabetes, it was correct around 65% of the time. \n- However, the recall is reasonably high around (0.80). This means that our model was able to capture around 80% of all actual diabetes cases. \n\nThe F1 score, a harmonic mean of precision and recall, is around 0.97 for class 0 and around 0.72 for class 1. The weighted average F1 score is around 0.94, in line with the overall accuracy.\n\nThis discrepancy in performance between classes is likely due to the imbalance in the original dataset. Class 0 (Non-diabetes) is the majority class and has more examples for the model to learn from.\n\n> However, the higher recall for class 1 (Diabetes) is promising. This is an essential aspect for a healthcare model, as missing actual positive cases (false negatives) can have serious implications. \n\n<BR>\n    \n<div style=\"border-radius: 10px; border: #16C2D5 solid; padding: 15px; background-color: #ffffff00; font-size: 110%; text-align: left;\">\n    📝 In summary, while our model performs well overall, it particularly excels with the majority class (non-diabetes). To enhance performance on the minority class (diabetes), we can further address class imbalance or adjust model parameters. Despite these areas for improvement, the model's ability to accurately identify a high percentage of actual diabetes cases is encouraging at this early stage of model development. Subsequent iterations and refinements are expected to enhance precision in diabetes predictions without compromising recall.\n</div>    \n    ","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"padding: 20px;color:white;margin:10;font-size:90%;text-align:left;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://w0.peakpx.com/wallpaper/957/661/HD-wallpaper-white-marble-white-stone-texture-marble-stone-background-white-stone.jpg)\"><b><span style='color:black'> 5. Feature Importance</span></b> </div>\n\nFinally, the importance of each feature is computed. This is the total decrease in node impurity (weighted by the probability of reaching that node, which is approximated by the proportion of samples reaching that node) averaged over all trees of the ensemble. **The feature importance gives insight into which features are most useful for making predictions.** The features are ranked by their importance and visualized using a bar plot.","metadata":{}},{"cell_type":"code","source":"# After fitting the model, we input feature names\nonehot_columns = list(grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(['gender', 'smoking_history']))\n\n# Then we add the numeric feature names\nfeature_names = ['age', 'BMI', 'HbA1c_level', 'blood_glucose_level', 'hypertension', 'heart_disease'] + onehot_columns\n\n# And now let's get the feature importances\nimportances = grid_search.best_estimator_.named_steps['classifier'].feature_importances_\n\n# Create a dataframe for feature importance\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n\n# Sort the dataframe by importance\nimportance_df = importance_df.sort_values('Importance', ascending=False)\n\n# Print the feature importances\nprint(importance_df)\n\n# Plot the feature importances\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=importance_df)\nplt.title('Feature Importances')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#16C2D5'>|</span> Intepret the results </b>\n\nThe feature importance results provide insight into which features are most influential in predicting diabetes using our **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">Random Forest Model</mark>**. The importance of a feature is calculated based on how much the tree nodes that use that feature reduce impurity across all trees in the forest.\n\n#### **<span style='color:#16C2D5'>Here are the key findings from the feature importance results:</span>**\n\n1. **`HbA1c_level`** is the most important feature with an importance of 0.44. HbA1c is a measure of the average levels of blood glucose over the past 2 to 3 months, so it's not surprising that it's a significant predictor of diabetes.\n\n2. **`The blood_glucose_level`** the second most important feature with an importance of 0.32. This aligns with medical knowledge, as blood glucose levels are directly used to diagnose diabetes.\n\n3. **`Age`** the third most important feature with an importance of 0.14. It's well known that the risk of type 2 diabetes increases as you get older.\n\n4. **`BMI`** comes fourth in terms of importance at 0.06. Body Mass Index is a key risk factor for diabetes, and its role is well documented in medical literature.\n\n5. Other features like **`hypertension`** and **`heart_disease`** show some importance (0.02 and 0.01, respectively), indicating that these health conditions might have some relevance in predicting diabetes, though not as significant as the top four factors.\n\n6. **`Smoking history`** ('smoking_history_non-smoker', 'smoking_history_past_smoker', 'smoking_history_current') and **`gender`** ('gender_Female', 'gender_Male') are shown to have minimal or zero importance in our model. This could be due to a number of reasons including that these factors may not be as influential in the development of diabetes or it could be a result of how the data was collected or structured.\n\n<br>\n\n<div style=\"border-radius:10px;border:#16C2D5 solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\"> \n    ⚠️ These results, however, should be interpreted with caution. The importance of a feature in a Random Forest model doesn't necessarily mean a casual relationship, and it is specific to this model and this dataset. Other models might find different results. Additionally, low importance doesn't mean that the feature is unimportant for predicting diabetes in general, it may just mean that the feature is not useful in the presence of the other features. A thorough feature analysis should be considered for a better understanding of the contribution of each feature in the prediction.\n\n</div> \n\n<br>\n\n> Overall, our findings do align well with medical knowledge and literature about risk factors for diabetes. The most important features are `blood-related` measurements, followed by `age` and `BMI`, with less importance seen for comorbid conditions like `hypertension` and `heart disease`.\n\n# <span style=\"color:#E888BB; font-size: 1%;\">SUMMARY</span>\n<div style=\"padding: 35px;color:white;margin:10;font-size:170%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://images.pexels.com/photos/3850689/pexels-photo-3850689.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\"><b><span style='color:black'>SUMMARY </span></b> </div>\n\n<br>\n\nThe analysis employed a **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">Random Forest classifier</mark>** to predict diabetes based on various health indicators and lifestyle factors. The model was trained and evaluated on a dataset of 100,000 records, and **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">Hyperparameter tuning</mark>** was performed to optimize the model's performance.\n\nThe model achieved an **accuracy of approximately 95.1%**, with precision of 0.98 for class 0 (non-diabetic) and 0.69 for class 1 (diabetic). It was also able to recall 96% of non-diabetic cases and 81% of diabetic cases correctly. The relatively high accuracy and balanced performance on both classes indicate that the model is well-tuned and robust.\n\nFeature importance analysis highlighted **`HbA1c_level`** and **`blood_glucose_level`** as the most critical factors in predicting **<span style='color:\t#16C2D5'>Diabetes</span>**. **`Age`** and **`BMI`** also showed significant importance. However, some features, such as **`smoking history`** and **`gender`**, had minimal or no impact on the model's predictions.\n\n# <span style=\"color:#E888BB; font-size: 1%;\">SUGGESTION</span>\n<div style=\"padding: 35px;color:white;margin:10;font-size:170%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://images.pexels.com/photos/3850689/pexels-photo-3850689.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\"><b><span style='color:black'>SUGGESTION</span></b> </div>\n\n<br>\n\n1. **<span style='color:#16C2D5'>Data Collection:</span>** If further data collection is possible, we could aim to gather more information about lifestyle factors and other potential diabetes risk factors not covered in this dataset. For instance, detailed diet information, physical activity level, family history of diabetes, and more precise information on heart disease or hypertension might improve the model's predictive capabilities.\n\n2. **<span style='color:#16C2D5'>Model Exploration:</span>** While the **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">Random Forest model</mark>** has performed well, it might be worth exploring other machine learning models. For instance, gradient boosting models like **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">XGBoost</mark>** or **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">LightGBM</mark>** could potentially offer improved performance.\n\n3. **<span style='color:#16C2D5'>Feature Engineering:</span>** More sophisticated feature engineering could potentially improve model performance. Interaction features, polynomial features, or other transformations might be worth exploring.\n\n4. **<span style='color:#16C2D5'>Model Interpretation:</span>**  To better understand the influence of each feature, we could use interpretability tools such as **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">SHAP</mark>** (SHapley Additive exPlanations) or **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">permutation feature importance</mark>**, which can offer a more nuanced view of feature importance than traditional feature importance based on impurity reduction.\n\n5. **<span style='color:#16C2D5'>Addressing Class Imbalance:</span>** Despite using **<mark style=\"background-color:#16C2D5;color:white;border-radius:5px;opacity:1.0\">SMOTE</mark>** to balance the classes, there is still room for improvement in the performance metrics for the minority class. **Other oversampling methods, undersampling methods, or cost-sensitive learning methods could be explored to improve the recall and precision for the minority class.**\n\n<br>\n    \n***\n\n<br>\n      \n<div style=\"text-align: center;\">\n   <span style=\"font-size: 4.5em; font-weight: bold; font-family: Arial;\">THANK YOU!</span>\n</div>\n\n<div style=\"text-align: center;\">\n    <span style=\"font-size: 5em;\">✔️</span>\n</div>\n\n<br>\n\n<div style=\"text-align: center;\">\n   <span style=\"font-size: 1.4em; font-weight: bold; font-family: Arial; max-width:1200px; display: inline-block;\">\n       If you discovered this notebook to be useful or enjoyable, I'd greatly appreciate any upvotes! Your support motivates me to regularly update and improve it. :-)\n   </span>\n</div>\n\n<br>\n\n<br>\n\n<div style=\"text-align: center;\">\n   <span style=\"font-size: 1.2em; font-weight: bold;font-family: Arial;\">@pannmie</span>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}}]}